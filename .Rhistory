shiny::runApp('C:/Users/ifeom/OneDrive/Old Drive/Desktop/Project/Final_project_UI')
library(Boruta)
# Feature Selection operation
set.seed(7)
boruta = Boruta(NewCleanSet$BULL_BEAR~.,data = NewCleanSet, doTrace = 4, maxRuns = 100)
library(moments)#A library to visualize our  features
#Histogram for Open
print(paste0("standard deviation: ", sd(CleanSet$OPEN)))
getwd()
EUR_USD_dataset = read.csv("Forex Dataset.csv") # historical EUR/USD dataset
library(dplyr) # A grammar library for data manipulation
EUR_USD_dataset = EUR_USD_dataset %>%
rename(
TICKVOLUME = TICKVOL,
VOLUME = VOL
)
View(EUR_USD_dataset)
library(quantmod)#A Library For Quantitative Financial Modelling
library(TTR)
#Simple moving average (SMA)
sma <-SMA(Op(EUR_USD_dataset),from='2018-03-01',to='2022-03-01')
EUR_USD_dataset$smaOPEN <- SMA(Op(EUR_USD_dataset),from='2018-03-01',to='2022-03-01')#for OPENING Price
sma <-SMA(Cl(EUR_USD_dataset),from='2018-03-01',to='2022-03-01')
EUR_USD_dataset$smaCLOSE <- SMA(Cl(EUR_USD_dataset),from='2018-03-01',to='2022-03-01')#for CLOSING Price
sma <-SMA(Hi(EUR_USD_dataset),from='2018-03-01',to='2022-03-01')
EUR_USD_dataset$smaHIGH <- SMA(Hi(EUR_USD_dataset),from='2018-03-01',to='2022-03-01')#for HIGH Price
sma <-SMA(Lo(EUR_USD_dataset),from='2018-03-01',to='2022-03-01')
EUR_USD_dataset$smaLOW <- SMA(Lo(EUR_USD_dataset),from='2018-03-01',to='2022-03-01')#for LOW Price
View(EUR_USD_dataset)
summary(EUR_USD_dataset)
library(DataExplorer) # A library to automate visual exploration of data
plot_intro(EUR_USD_dataset)
plot_missing(EUR_USD_dataset)
library(mice) # A library for Multivariate Imputation by Chained Equations
md.pattern(EUR_USD_dataset)
mice_imputes = mice(EUR_USD_dataset, m = 20, maxit = 40, seed = 10)
mice_imputes$method
EUR_USD_dataset = complete(mice_imputes, 20)
md.pattern(EUR_USD_dataset)
summary(EUR_USD_dataset) #confirm absence of missing data and gain insights on the data
View(EUR_USD_dataset)
#Missing Data Present
#since number is negligible, we effect complete case analysis
CleanSet = na.omit(EUR_USD_dataset)
summary(CleanSet)#confirm absence of missing data and gain insights on the data
plot_missing(CleanSet)
View(CleanSet)
str(CleanSet)
CleanSet$TICKVOLUME = as.numeric(as.integer(CleanSet$TICKVOLUME))
CleanSet$VOLUME = as.numeric(as.integer(CleanSet$VOLUME))
CleanSet$SPREAD = as.numeric(as.integer(CleanSet$SPREAD))
CleanSet$BULL_BEAR = as.numeric(as.integer(CleanSet$BULL_BEAR))
str(CleanSet)
library(moments)#A library to visualize our  features
#Histogram for Open
print(paste0("standard deviation: ", sd(CleanSet$OPEN)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$OPEN),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$OPEN, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$OPEN)
std<-sqrt
#Histogram for High
print(paste0("standard deviation: ", sd(CleanSet$HIGH)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$HIGH),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$HIGH, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$HIGH)
std<-sqrt(var(CleanSet$HIGH))
#Histogram for Low
print(paste0("standard deviation: ", sd(CleanSet$LOW)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$LOW),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$LOW, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$LOW)
std<-sqrt(var(CleanSet$LOW))
#Histogram for Close
print(paste0("standard deviation: ", sd(CleanSet$CLOSE)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$CLOSE),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$CLOSE, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$CLOSE)
std<-sqrt(var(CleanSet$CLOSE))
#Histogram for TickVolume
print(paste0("standard deviation: ", sd(CleanSet$TICKVOLUME)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$TICKVOLUME),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$TICKVOLUME, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$TICKVOLUME)
std<-sqrt(var(CleanSet$TICKVOLUME))
#Histogram for Volume
print(paste0("standard deviation: ", sd(CleanSet$VOLUME)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$VOLUME),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
#hist(CleanSet$VOLUME, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$VOLUME)
std<-sqrt(var(CleanSet$VOLUME))
#Histogram for Spread
print(paste0("standard deviation: ", sd(CleanSet$SPREAD)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$SPREAD),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$SPREAD, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$SPREAD)
std<-sqrt(var(CleanSet$SPREAD))
#Histogram for Bull_Bear
print(paste0("standard deviation: ", sd(CleanSet$BULL_BEAR)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$BULL_BEAR),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$BULL_BEAR, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$BULL_BEAR)
std<-sqrt(var(CleanSet$BULL_BEAR))
#Histogram for smaOpen
print(paste0("standard deviation: ", sd(CleanSet$smaOPEN)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$smaOPEN),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$smaOPEN, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$smaOPEN)
std<-sqrt(var(CleanSet$smaOPEN))
#Histogram for smaHigh
print(paste0("standard deviation: ", sd(CleanSet$smaHIGH)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$smaHigh),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$smaHIGH, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$smaHigh)
#Histogram for smaLow
print(paste0("standard deviation: ", sd(CleanSet$smaLOW)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$smaLOW),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$smaLOW, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$smaLOW)
std<-sqrt(var(CleanSet$smaLOW))
#Histogram for smaClose
print(paste0("standard deviation: ", sd(CleanSet$smaCLOSE)))
print(paste0("Kurtosis: ", round(kurtosis(CleanSet$smaCLOSE),2)))
options(repr.plot.width = 4, repr.plot.height = 4)
hist(CleanSet$smaCLOSE, breaks=100, prob=TRUE) # Make it a probability distribution
m<-mean(CleanSet$smaCLOSE)
std<-sqrt(var(CleanSet$smaCLOSE))
# Overlay a standard normal distribution
curve(dnorm(x, mean=m, sd=std), col="darkblue", lwd=2, add=TRUE )
library(ggplot2)#A library for plotting our scatterplot
#OPEN
plot(CleanSet$OPEN, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. OPEN',
xlab='BULL_BEAR', ylab='OPEN')
#smaOPEN
plot(CleanSet$smaOPEN, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. smaOPEN',
xlab='BULL_BEAR', ylab='smaOPEN')
#CLOSE
plot(CleanSet$CLOSE, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. CLOSE',
xlab='BULL_BEAR', ylab='CLOSE')
#smaCLOSE
plot(CleanSet$smaCLOSE, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. smaCLOSE',
xlab='BULL_BEAR', ylab='smaCLOSE')
#HIGH
plot(CleanSet$HIGH, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. HIGH',
xlab='BULL_BEAR', ylab='HIGH')
#smaHIGH
plot(CleanSet$smaHIGH, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. smaHIGH',
xlab='BULL_BEAR', ylab='smaHIGH')
#TICKVOLUME
plot(CleanSet$TICKVOLUME, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. TICKVOLUME',
xlab='BULL_BEAR', ylab='TICKVOLUME')
#VOLUME
plot(CleanSet$VOLUME, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. VOLUME',
xlab='BULL_BEAR', ylab='VOLUME')
#SPREAD
plot(CleanSet$SPREAD, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. SPREAD',
xlab='BULL_BEAR', ylab='SPREAD')
#LOW
plot(CleanSet$LOW, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. LOW',
xlab='BULL_BEAR', ylab='LOW')
#smaLOW
plot(CleanSet$smaLOW, CleanSet$BULL_BEAR, pch=16, col='steelblue',
main='BULL_BEAR vs. smaLOW',
xlab='BULL_BEAR', ylab='smaLOW')
library(psych) # multivariate analysis
NewCleanSet = CleanSet[,-c(1,5,11)]#data set without date,close and smaCLOSE
corr.test(NewCleanSet)
# Check the correlation significance levels
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$OPEN)$p.value
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$HIGH)$p.value
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$LOW)$p.value
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$TICKVOLUME)$p.value
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$VOLUME)$p.value
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$SPREAD)$p.value
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$smaOPEN)$p.value
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$smaHIGH)$p.value
cor.test(NewCleanSet$BULL_BEAR,NewCleanSet$smaLOW)$p.value
library(mlbench)
library(caret)
str(NewCleanSet)
sapply(NewCleanSet,class)
NewCleanSet$BULL_BEAR = as.factor(as.integer(NewCleanSet$BULL_BEAR))
summary(NewCleanSet)
library(Boruta)
# Feature Selection operation
set.seed(7)
boruta = Boruta(NewCleanSet$BULL_BEAR~.,data = NewCleanSet, doTrace = 4, maxRuns = 100)
plot(boruta, las = 2, cex.axis = 0.5)
# reclassify features that were designated as tentative
bor = TentativeRoughFix(boruta)
plot(bor, las = 2, cex.axis = 0.5)
attStats(boruta)
print(bor)
# create object and limit features to only the 5 confirmed features from the boruta analysis + the dependent variable
col_order2 = c("OPEN","HIGH","LOW","smaOPEN","smaLOW","BULL_BEAR")
NewSet = NewCleanSet[, col_order2]
View(NewSet)
str(NewSet)
levels(NewSet)
#LOW/BEARISH TREND
LOW = NewSet[NewSet$BULL_BEAR == 0,]
#HIGH/BULLISH TREND
HIGH = NewSet[NewSet$BULL_BEAR == 1,]
set.seed(7)
validationIndex = createDataPartition(NewSet$BULL_BEAR , p=0.70, list = FALSE)
#select 30% of the data for validation
validation = NewSet[-validationIndex,]
#use the remaining 70% of data to train the model
dataset = NewSet[validationIndex,]
# Check that the distribution of the dependent variable is similar in train and test sets
prop.table(table(NewSet$BULL_BEAR))
prop.table(table(dataset$BULL_BEAR))
prop.table(table(validation$BULL_BEAR))
trainControl = trainControl(method = "repeatedcv", number = 10, repeats = 3)
metric = "Accuracy"
#1 LG
set.seed(7)
fit.glm = train(BULL_BEAR~., data=dataset, method="glm", metric=metric, trControl=trainControl)
#2 LDA
set.seed(7)
fit.lda = train(BULL_BEAR~., data=dataset, method="lda", metric=metric, trControl=trainControl)
#3 KNN
set.seed(7)
fit.knn = train(BULL_BEAR~., data=dataset, method="knn", metric=metric, trControl=trainControl)
#4 Random Forest
set.seed(7)
fit.rf = train(BULL_BEAR~., data=dataset, method="rf", metric=metric, trControl=trainControl)
#5 SVM
set.seed(7)
fit.svmLinear = train(BULL_BEAR~., data=dataset, method="svmLinear", metric=metric, trControl=trainControl)
#6 Naive Bayes
set.seed(7)
fit.naive_bayes = train(BULL_BEAR~., data=dataset, method="naive_bayes", metric=metric, trControl=trainControl)
#7 Multi Layer Perceptron
set.seed(7)
fit.mlp = train(BULL_BEAR~., data=dataset, method="mlp", metric=metric, trControl=trainControl)
#8 Neural Network
set.seed(7)
fit.pcaNNet = train(BULL_BEAR~., data=dataset, method="pcaNNet", metric=metric, trControl=trainControl)
# Compare algorithms
results = resamples(list(LG=fit.glm, LDA=fit.lda, KNN=fit.knn,RF=fit.rf,  SVM=fit.svmLinear, NB = fit.naive_bayes, MLP = fit.mlp, NN=fit.pcaNNet))
summary(results)
dotplot(results)
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "Accuracy"
#LG
set.seed(7)
grid = expand.grid(.parameter = seq(1,10, by=1))
fit.LG_tuned = train(BULL_BEAR~.,data = dataset, method="glm", metric=metric, tuneGrid=grid, trControl=trainControl)
fit.LG_tuned
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "Accuracy"
#LDA
set.seed(7)
grid = expand.grid(.parameter = seq(1,10, by=1))
fit.lda_tuned = train(BULL_BEAR~.,data = dataset, method="lda", metric=metric, tuneGrid=grid, trControl=trainControl)
fit.lda_tuned
set.seed(7)
x = dataset[,1:6]
preprocessParam = preProcess(x)
x = predict(preprocessParam, x)
# prepare the validation dataset
set.seed(7)
# transform the validation dataset
validationX = predict(preprocessParam, validation[,1:6])
glmPredict = predict(fit.glm, newdata = validation)
confusionMatrix(glmPredict, validation$BULL_BEAR)
library(PRROC)
sapply(glmPredict, class)
sapply(validation, class)
pred = as.numeric(as.character(glmPredict))
valid = as.character(as.factor(validation$BULL_BEAR))
valid = as.numeric(as.character(validation$BULL_BEAR))
prroc_obj = roc.curve(scores.class0 = pred, weights.class0 = valid, curve = TRUE)
plot(prroc_obj)
runApp('C:/Users/ifeom/OneDrive/Old Drive/Desktop/Project/Final_project_UI')
